{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPQl3CldsuOzEON8Xl1WZlO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/path-0f-misantrope/most_uselles_musor_final/blob/main/modellll.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_L2KwtOVyeX"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n"
      ],
      "metadata": {
        "id": "trz-FqsEV02c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/разметка_готовая.csv')\n",
        "df.info()"
      ],
      "metadata": {
        "id": "efCKPqalV4Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.loc[\n",
        "    (df[\"Понравилось выполнение заявки\"] == 1) &\n",
        "    (df[\"Нравится качество выполнения заявки\"] == 0),\n",
        "    \"Нравится качество выполнения заявки\"\n",
        "] = 1\n",
        "\n",
        "# Удаляем колонку \"Понравилось выполнение заявки\"\n",
        "df.drop(columns=[\"Понравилось выполнение заявки\"], inplace=True)\n"
      ],
      "metadata": {
        "id": "rbPz6-ymWXdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "U8Hb7tc1XYfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text_col = 'comment'\n",
        "label_cols = [\n",
        "    'Нравится скорость отработки заявок',\n",
        "    'Нравится качество выполнения заявки',\n",
        "    'Нравится качество работы сотрудников',\n",
        "    'Вопрос решен'\n",
        "]\n",
        "df = df[[text_col] + label_cols].dropna()\n",
        "df = df[df[label_cols].sum(axis=1) > 0]"
      ],
      "metadata": {
        "id": "wh2qmAX5Y0qM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "IZkRhoELY8Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df[text_col].tolist(),\n",
        "    df[label_cols].values,\n",
        "    test_size=0.2,\n",
        "    random_state=52\n",
        ")\n"
      ],
      "metadata": {
        "id": "dFVQ4mJ5Z4IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "class CommentDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = tokenizer(self.texts[idx],\n",
        "                             truncation=True,\n",
        "                             padding='max_length',\n",
        "                             max_length=128,\n",
        "                             return_tensors=\"pt\")\n",
        "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "train_dataset = CommentDataset(X_train, y_train)\n",
        "test_dataset = CommentDataset(X_test, y_test)\n"
      ],
      "metadata": {
        "id": "1D93bEwZZ7TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\",\n",
        "    num_labels=len(label_cols),\n",
        "    problem_type=\"multi_label_classification\"\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=8,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    #evaluation_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZZzE1iiAaEjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = trainer.predict(test_dataset)\n",
        "y_pred = (preds.predictions > 0.5).astype(int)\n",
        "print(classification_report(y_test, y_pred, target_names=label_cols))\n"
      ],
      "metadata": {
        "id": "f7taGiCUaFOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "                precision    recall  f1-score   support\n",
        "\n",
        "  Нравится скорость отработки заявок       0.99      0.96      0.97       162\n",
        " Нравится качество выполнения заявки       0.81      0.78      0.80        55\n",
        "Нравится качество работы сотрудников       0.73      0.51      0.60        43\n",
        "                        Вопрос решен       0.96      0.97      0.97       250\n",
        "\n",
        "                           micro avg       0.94      0.91      0.92       510\n",
        "                           macro avg       0.87      0.81      0.83       510\n",
        "                        weighted avg       0.94      0.91      0.92       510\n",
        "                         samples avg       0.94      0.92      0.92       510\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "nbnimzRq1rSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    probs = torch.sigmoid(torch.tensor(logits)).numpy()\n",
        "\n",
        "    # Binarize с порогом 0.5\n",
        "    preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "    auc = roc_auc_score(labels, probs, average=\"macro\")\n",
        "    f1 = f1_score(labels, preds, average=\"macro\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"f1\": f1,\n",
        "        \"roc_auc\": auc\n",
        "    }\n",
        "print(compute_metrics((preds.predictions, y_test)))"
      ],
      "metadata": {
        "id": "LPif8IwMePL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'accuracy': 0.8246153846153846, 'f1': 0.8460273006101483, 'roc_auc': np.float64(0.952922541104103)\n"
      ],
      "metadata": {
        "id": "n2pkc-G810QM"
      }
    }
  ]
}